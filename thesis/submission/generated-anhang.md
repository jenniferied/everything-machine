\newpage

# Anhang

## Anhang A: Verzeichnis der Journal-Einträge

Die folgende Tabelle gibt einen Überblick über die im Rahmen dieser Arbeit erstellten Journal-Einträge. Jeder Eintrag dokumentiert ein Experiment oder eine Arbeitsphase und wurde im Anschluss einer strukturierten KI-gestützten Reflexion unterzogen (siehe Anhang B).

| Nr. | Datum | Titel | Tools | Modalität |
|-----|-------|-------|-------|-----------|
| 1 | 15. November 2025 | Referenzen für Kepler sammeln | Pinterest, Google Drive | --- |
| 2 | 16. November 2025 | ComfyUI Consistent Character Workflow | ComfyUI, Flux, IP-Adapter | Bild |
| 3 | 18. November 2025 | Marble von Worldlabs | Marble, Claude | 3D |
| 4 | 19. November 2025 | Website mit Cursor aufsetzen | Cursor, Claude | Code |
| 5 | 11. Januar 2026 | KI wirklich verstehen -- Teil 1 | Claude (Voice) | Dialog |
| 6 | 12. Januar 2026 | Suno-Experiment | Suno AI | Audio |
| 7 | 25. Januar 2026 | Kepler-Recherche, zweiter Versuch | Claude, Semantic Scholar, GitHub | Recherche |

Die vollständigen Journal-Einträge mit Abbildungen sind auf der Projektwebsite einsehbar: https://jnfrmeier.github.io/everything-machine/

\newpage

## Anhang B: Journal-Einträge und Reflexionsinterviews

Die folgenden Texte und Transkripte dokumentieren die Journal-Einträge und die strukturierten Reflexionen, die nach jedem Eintrag im Dialog mit Claude (Anthropic) entstanden sind. Eingebettete Audio- und Videomedien wurden für die Druckversion entfernt; die vollständigen Einträge finden sich auf der Projektwebsite.

### Journal-Eintrag 1: Referenzen für Kepler sammeln (15. November 2025)

#### Der Start

Im Rahmen der Artistic Research-Vorlesung bei Prof. Dr. Aristotelis Hadjakos an der TH OWL bin ich zum ersten Mal mit dem Konzept der Artistic Research in Berührung gekommen. Borgdorffs Idee – Forschung *in* und *durch* die Kunst, bei der die künstlerische Praxis selbst die Forschungsmethode ist – klang faszinierend, aber auch einschüchternd. Was ist AR überhaupt? Ist das, was ich mache, Kunst? Ich war unsicher, aber das Interesse war da.

Kepler ist Gavins Musikprojekt – eine digitale Künstleridentität, ein Charakter mit Voxel-Maske, der zwischen Mensch und Avatar existiert. KI-Tools interessieren mich gerade, und die Idee lag nahe: Was passiert, wenn ich generative KI-Systeme nutze, um Kepler weiterzuentwickeln? Wie verändert das den kreativen Prozess?

Hadjakos hat betont, dass Artistic Research frei sein soll – der Versuch, sich von starren Strukturen zu lösen und erstmal zu machen. Also haben wir heute angefangen: Referenzen sammeln, bevor ich mit der tatsächlichen Bildgenerierung beginne. Ich wollte verstehen, welche visuellen Stile und Darstellungen Gavin gefallen und gut funktionieren könnten.

#### Pinterest-Recherche

Ich habe zusammen mit Gavin Pinterest durchforstet und verschiedene Boards angelegt – von Cover-Inspiration über Kleidung bis hin zu Vibes für spezifische Songs. Es ist erstaunlich, wie viele coole "Ästhetiken" sich auf Pinterest wiederfinden. Manche Looks sind hyperrealistisch, andere eher stilisiert. Wir haben versucht, eine gute Mischung zu finden, die zu der Vision von Kepler passt. Dabei sind uns bestimmte wiederkehrende Muster aufgefallen: dunkel, strukturiert, maskenhaft – eine visuelle Sprache, die schon durch die Auswahl entsteht, nicht erst durch Generierung.

**Mein Pinterest-Board:** [Kepler x Me](https://uk.pinterest.com/jennifer_ied/kepler-x-me/)

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/f42e1939dd3ca4b90215ee26aa5fc4d1.png}\\[2pt]{\small\color{darktext!70} Pinterest Board Übersicht}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/fa078f5b6ef517de7983afb72f39af67.png}\\[2pt]{\small\color{darktext!70} Pinterest Song Bescheid}
\end{minipage}
\end{figure}
```
#### Google Ordner

Parallel dazu habe ich einen Google Drive-Ordner eingerichtet, um Daten mit Gavin zu teilen. Wir beide arbeiten an Kepler-bezogenen Projekten, und es macht Sinn, unsere Ressourcen zu bündeln. Ich habe die gesammelten Referenzbilder, bereits erstellte Cover und 3D-Daten hochgeladen. Gavin hat ebenfalls Material beigesteuert – Fotos, existierende Medien die von Anderen für Kepler erstellt worden sind (Cover, Artwork) und Konzeptideen, die ich noch nicht hatte. Der Ordner wurde zum zentralen Archiv für alles, was es zu Kepler bisher gibt: was wir selbst gemacht haben, was andere beigetragen haben, und was wir als Referenz nutzen wollen.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/36e8d354d95f492b91d361ee29356672.png}\\[2pt]{\small\color{darktext!70} Google Drive Ordner}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/d5cd0df2bd312f3b74a58f89a096ab35.png}\\[2pt]{\small\color{darktext!70} Google Drive Referenzen}
\end{minipage}
\end{figure}
```

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du in dieser Session verwendet?
\end{claudemsg}
\begin{usermsg}
Pinterest, Google Drive und keine KI-Tools so far.
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Alle drei (Referenzen sammeln, Zusammenarbeit starten, Inspiration finden).
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? Was war neu, was ist weggefallen? \emph{(Workflow)}
\end{claudemsg}
\begin{usermsg}
Es war ganz wie gewohnt, wir haben einfach angefangen zu kollaborieren, in diesem Fall digital, und unsere Ressourcen zu sammeln.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle in diesem Prozess erlebt? \emph{(Autorschaft)}
\end{claudemsg}
\begin{usermsg}
Kollaboration mit Gavin. Wir haben parallel gearbeitet, jeder an seinem Rechner. Ich habe die Struktur vorgegeben, also Ordner und Boards, er hat Material beigesteuert. Keine KI involviert, rein menschliche Kuration und Abstimmung.
\end{usermsg}
\begin{claudemsg}
Wie hat die schnelle KI-Ausgabe deinen Prozess beeinflusst? \emph{(Iteration)}
\end{claudemsg}
\begin{usermsg}
Keine KI verwendet, daher keine KI-Iteration. Aber der Prozess selbst war iterativ: Wir haben gesammelt, sortiert, verworfen, neu gesucht. Der Unterschied: Es war langsamer, aber alles war bewusst gewählt.
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen und was hast du daraus gelernt? \emph{(Scheitern)}
\end{claudemsg}
\begin{usermsg}
Es war relativ reibungslos auf meiner Seite, aber Gavin wurde etwas ungeduldig, da ich aus Erfahrung mehr Struktur wollte (es fehlen immer noch ein paar Sachen, die er hochladen wollte, naja...).
\end{usermsg}
\begin{claudemsg}
Hat sich durch die Arbeit eine eigene visuelle/stilistische Sprache entwickelt? \emph{(Ästhetik)}
\end{claudemsg}
\begin{usermsg}
Noch nicht direkt, das war Vorarbeit. Aber beim Sammeln sind mir bestimmte wiederkehrende Ästhetiken aufgefallen, die zu Kepler passen: dunkel, strukturiert, maskenhaft. Die visuelle Sprache entsteht hier durch Kuration, nicht durch Generierung.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Du hast erwähnt, dass du dir Gedanken über Urheberrechte machst – wenn man fremde Bilder als Stilreferenz nutzt. Hat das Sammeln der Referenzen dein Verhältnis zu 'originalem' versus 'inspiriertem' Schaffen verändert?
\end{claudemsg}
\begin{usermsg}
Das Sammeln von Referenzen hat mir gezeigt, dass der kreative Prozess schon beim Kuratieren beginnt, nicht erst beim Generieren. Ob die Bilder, die mich inspirieren, selbst KI-generiert sind, spielt für die Inspiration keine Rolle. Aber für die Weiterverwendung als Trainingsdaten schon.
\end{usermsg}
```

\newpage

### Journal-Eintrag 2: ComfyUI Consistent Character Workflow (16. November 2025)

Heute habe ich mich zum ersten Mal richtig mit dem ComfyUI Consistent Character Workflow beschäftigt. Das Ziel war klar: Ich wollte herausfinden, ob ich damit konsistente Darstellungen von Kepler über verschiedene Bilder hinweg erzeugen kann, ohne jedes Mal von vorne anfangen zu müssen.

Zuerst habe ich versucht, bestehende Bilder als Referenzen zu verwenden. Das hat teilweise funktioniert, aber es gab einige Probleme, die schnell klar wurden. Wenn ich zum Beispiel ein Bild hochgeladen habe, auf dem Kepler einen Basketball hält, und dann in den Prompts eine Maske hinzugefügt habe, wurde die Maske zwar generiert, aber der Basketball blieb einfach drin. Das Modell hat offenbar nicht verstanden, dass bestimmte Elemente entfernt werden sollten, wenn neue hinzukommen. Ähnlich war es mit Gesichtern – wenn ich ein Bild mit einem bestimmten Gesichtsausdruck als Referenz genommen habe, wurden bei der Generierung manchmal zusätzliche Gesichter eingefügt, als Platzhalter für Ergebnisse, die das System noch nicht richtig interpretieren konnte.

Nach ein paar frustrierenden Versuchen habe ich beschlossen, eine eigene Referenz zu erstellen. Ich habe Kepler auf einem neutralen grauen Hintergrund fotografiert, ohne störende Elemente, ohne Basketball, ohne komplexe Hintergründe. Nur er, klar und deutlich. Dann habe ich die Prompts leicht angepasst, um zu sehen, wie sich das auf die Ergebnisse auswirkt.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/c6e839bfe070ba830bca79a350ec9bc7.png}
\caption{Referenzbild von Kepler auf neutralem Hintergrund}
\end{figure}
```
Die Ergebnisse waren gemischt. Einige Bilder sahen wirklich gut aus – der Charakter war konsistent, die Details stimmten, und die Maske wurde korrekt dargestellt. Andere waren weniger überzeugend. Manchmal verlor der Workflow die Konsistenz komplett, besonders bei unterschiedlichen Posen oder wenn ich versucht habe, komplexere Szenen zu generieren. Es scheint, als ob das System am besten funktioniert, wenn die Referenzbilder möglichst klar und unkompliziert sind.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/9f1a00b4e8ba9570474e67aa8d6eea69.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 1}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/0f9d67c6e8f86bffe86e39d4a0126789.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 2}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/cf06af54acd2340a567091d4c11d91f3.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 3}
\end{minipage}

\vspace{4pt}

\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/4a391f5ed852bbfb5b178acbf1da3d82.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 4}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/c84deeec050394a8d474582135b56a28.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 5}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/21582a866de77f4d824d9656f1c93842.png}\\[2pt]{\small\color{darktext!70} Generiertes Kepler-Bild 6}
\end{minipage}
\end{figure}
```
#### ComfyUI und Flux

Der ComfyUI Consistent Character Workflow basiert auf mehreren Technologien, die zusammenarbeiten, um konsistente Charakterdarstellungen zu ermöglichen. ComfyUI selbst ist eine grafische Benutzeroberfläche für Stable Diffusion, die es ermöglicht, komplexe Bildgenerierungsprozesse durch modulare Workflows zu steuern. Statt Code zu schreiben, verbindet man Nodes miteinander, die verschiedene Funktionen repräsentieren – von der Bildladung über die Prompt-Verarbeitung bis hin zur finalen Ausgabe.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/eb6462d57cd5ce87c783b7b30a443d01.png}\\[2pt]{\small\color{darktext!70} ComfyUI Workflow Screenshot 1}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/1ad179a3e7df5b7d4000c4acb6de4bf9.png}\\[2pt]{\small\color{darktext!70} ComfyUI Workflow Screenshot 2}
\end{minipage}
\end{figure}
```
Der Consistent Character Workflow nutzt dabei hauptsächlich das Flux-Modell, ein von Black Forest Labs entwickeltes Bildgenerierungsmodell, das besonders gut darin ist, konsistente Darstellungen zu erzeugen. Flux funktioniert ähnlich wie Stable Diffusion, verwendet aber eine andere Architektur, die es ermöglicht, feinere Details und bessere Konsistenz über mehrere Bilder hinweg zu erreichen.

#### IP-Adapter und ControlNet

Ein wichtiger Teil des Workflows ist die Verwendung von IP-Adapter oder ähnlichen Technologien, die es ermöglichen, Referenzbilder in den Generierungsprozess einzubinden. Diese Adapter analysieren das Referenzbild und extrahieren bestimmte Merkmale – Gesichtszüge, Kleidung, Stil – und versuchen dann, diese Merkmale in neuen Generierungen beizubehalten. Das funktioniert durch sogenannte Attention-Mechanismen, bei denen das Modell lernt, welche Teile des Referenzbilds wichtig sind und welche ignoriert werden können.

ControlNet ist ein weiteres wichtiges Werkzeug in diesem Workflow. Es ermöglicht es, bestimmte Aspekte der Generierung zu kontrollieren – zum Beispiel die Pose eines Charakters oder die Tiefenstruktur einer Szene. ControlNet funktioniert, indem es zusätzliche Informationen aus einem Kontrollbild extrahiert und diese Informationen dann in den Generierungsprozess einfließen lässt. Wenn ich zum Beispiel ein Bild mit einer bestimmten Pose habe, kann ControlNet diese Pose erkennen und sicherstellen, dass der generierte Charakter in derselben Pose erscheint.

Das Problem, das ich mit dem Basketball hatte, liegt wahrscheinlich daran, dass das System nicht wirklich "versteht", was es generiert. Es erkennt Muster und Merkmale, aber es hat kein semantisches Verständnis davon, dass ein Basketball und eine Maske möglicherweise nicht gleichzeitig vorhanden sein sollten. Das Modell sieht einfach verschiedene visuelle Elemente und versucht, sie zu kombinieren, ohne die logischen Zusammenhänge zu berücksichtigen.

#### Face Detailer — und das Problem

Face Detailer ist ein zusätzliches Tool, das speziell darauf ausgelegt ist, Gesichter zu verbessern. Es funktioniert, indem es das generierte Bild analysiert, Gesichter erkennt und dann diese Bereiche mit einem speziell trainierten Modell nachbearbeitet. Das kann helfen, Details zu schärfen und realistischere Gesichtszüge zu erzeugen, aber es kann auch zu Problemen führen, wenn das System Gesichter erkennt, wo keine sein sollten, oder wenn es versucht, Gesichter zu "reparieren", die eigentlich korrekt waren.

Der gesamte Workflow ist im Grunde eine Kette von Transformationen. Zuerst wird das Referenzbild analysiert und seine Merkmale werden extrahiert. Dann wird ein Text-Prompt verarbeitet, der beschreibt, was generiert werden soll. Diese Informationen werden kombiniert und durch das Flux-Modell geleitet, das Schritt für Schritt ein neues Bild generiert. ControlNet kann dabei zusätzliche Kontrolle bieten, und Face Detailer kann am Ende noch einmal über die Gesichter gehen, um sie zu verbessern.

Das Problem ist, dass jeder dieser Schritte Fehler einführen kann. Wenn die Referenz nicht klar genug ist, werden die extrahierten Merkmale ungenau. Wenn der Prompt zu komplex ist, kann das Modell verschiedene Interpretationen haben. Und wenn ControlNet oder Face Detailer falsch interpretieren, was sie sehen sollen, entstehen Artefakte wie zusätzliche Gesichter oder nicht entfernte Objekte.

Die Lösung, die ich gefunden habe – eine klare Referenz auf neutralem Hintergrund – funktioniert, weil sie dem System weniger Raum für Fehlinterpretationen lässt. Wenn nur der Charakter zu sehen ist, ohne störende Elemente, kann das Modell sich auf die wichtigen Merkmale konzentrieren. Aber selbst dann ist die Konsistenz nicht garantiert, besonders wenn man versucht, den Charakter in verschiedenen Posen oder Kontexten zu zeigen.

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du in dieser Session verwendet?
\end{claudemsg}
\begin{usermsg}
ComfyUI mit einem Network von Mickmumpitz (Füge einen Link zu seinem YT Channel hinzu).
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Ich wollte schauen, ob ich Album-Cover- oder Konzeptbilder erstellen konnte von Kepler. Mir war aber wichtig, dass eine Konsistenz besteht und es ästhetisch aussieht. Ich war auch etwas von dem ComfyUI Workflow und dem Konzept überfordert, deswegen wollte ich das als Plattform nutzen, um das Ganze zu lernen.
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? (Workflow)
\end{claudemsg}
\begin{usermsg}
Es ist ein komplett neuer Workflow mit ComfyUI zu arbeiten. Ich glaube, ich muss wiederkehren, um das Ganze besser zu verstehen, und vielleicht nicht mit Kepler als Charakter. Denn ich habe gleich gesehen, dass er besonders schwierig ist. Er ist jetzt kein Mensch per se, und sein Stil könnte schwierig zu replizieren sein, da er kein Gesicht hat, eine Maske trägt und in der Version, in der ich sie ihnen zeigen möchte, auch noch voxelized ist.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle erlebt? (Autorschaft)
\end{claudemsg}
\begin{usermsg}
Ich war Lernende und experimentierte eher als ich das Tool wirklich verwendete. Ich hatte mich ja an dem Netzwerk einer anderen Person orientiert.
\end{usermsg}
\begin{claudemsg}
Wie hat die schnelle KI-Ausgabe deinen Prozess beeinflusst? (Iteration)
\end{claudemsg}
\begin{usermsg}
Es war gar nicht so schnell, da ich es tatsächlich lokal auf meinem M1-MacBook Pro Max 64 GB ausprobiert habe. Auch der Prozess, das Ganze zu installieren, hat länger gedauert und hat viele Gigabyte benötigt. Also war es etwas frustrierend.
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen? (Scheitern)
\end{claudemsg}
\begin{usermsg}
Es war sehr inkonsistent. Die Prompts haben nicht das erfüllt, was sie erfüllen sollten. Der Basketball wurde nicht entfernt, als ich ein Bild mit Basketball eingefügt habe. Besonders frustrierend war, dass Gesichter in Keplers Maske rein interpretiert worden sind. Auch als ich negative Prompts dafür eingefügt habe. Ich glaube, das wird ein größeres Problem.
\end{usermsg}
\begin{claudemsg}
Hat sich eine eigene visuelle Sprache entwickelt? (Ästhetik)
\end{claudemsg}
\begin{usermsg}
Nein, denn so weit bin ich gar nicht erst gekommen in diesem Anlauf. Denn es haben Sachen ja auch nicht funktioniert, und das, was herauskam, sah schon sehr generisch KI-mäßig aus.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Du hast erwähnt, dass das System Gesichter in Keplers Maske interpretiert hat – obwohl er bewusst kein Gesicht haben soll. Wie fühlt es sich an, wenn die KI deine künstlerische Intention so grundlegend 'missversteht'?
\end{claudemsg}
\begin{usermsg}
Es ist einfach frustrierend. Am Ende des Tages ist es ein technisches Problem.\par
Ich wollte ja genau durch die Verwendung von KI neue Möglichkeiten schaffen, die intuitiv sind und Spaß machen und auch Zeit einsparen, um Dinge zu kreieren, ohne jetzt mir ganz die Arbeit abzunehmen.\par
Aber es zeigt halt, dass wir noch nicht ganz da sind und die Art Directability noch nicht da ist. Und das würde ich mir eben wünschen. Aber es sind halt keine magischen Tools, so wie es sich anfühlt.\par
Für viele, wenn sie das erste Mal ChatGPT benutzen, sind es einfach nur Werkzeuge, die mehr oder weniger gut für den Anwendungsfall funktionieren. Es hätte zufällig gut funktionieren können. Es würde viel besser funktionieren, wenn es sich um eine realistische, oder Anime-style, oder Cartoon-style Figur handeln würde. Aber dem ist nicht so. Kepler hat eben kein sichtbares Gesicht, aber viele der Trainingsdaten enthalten Gesichter und deswegen bleiben sie hartnäckig dort.
\end{usermsg}
```

\newpage

### Journal-Eintrag 3: Marble von Worldlabs (18. November 2025)

Heute habe ich Marble von Worldlabs getestet, ein neues Tool, das unter [marble.worldlabs.ai](https://marble.worldlabs.ai) verfügbar ist. Ich hatte davon gehört, dass es sich um ein Tool zur Erstellung von 3D-Welten handelt, und war neugierig, wie es sich von anderen Tools unterscheidet, die ich bisher verwendet habe.

Marble ist im Grunde eine browserbasierte Plattform, die es ermöglicht, interaktive 3D-Welten zu erstellen und zu erkunden. Das Besondere daran ist, dass man keine komplexe Software installieren muss – alles läuft im Browser, was den Einstieg deutlich erleichtert. Die Oberfläche ist relativ intuitiv gestaltet, auch wenn es am Anfang etwas dauert, bis man sich zurechtfindet.

Ich habe vier verschiedene Welten erstellt, um zu testen, wie das Tool mit unterschiedlichen Input-Strategien umgeht – von konkreten Fotos bis zu abstrakten Vibes.

#### Welt 1

Die erste Welt war meine erste Welt, die ich erstellt habe. Sie basiert auf einem Bild von meiner Freundin und mir in dem Badezimmer eines Soviet-inspirierten Cafés, mit alten Artefakten aus der Soviet-Zeit. Die Welt ist sehr creepy und verzerrt – man kann nur noch eine Person erkennen, die fast gruselige Proportionen hat. Das sehr interessante ist, dass das System das Bild, welches ein Spiegelbild ist, als Spiegel erkannt hat und quasi die gespiegelten Personen in den Raum gebracht hat. Es hat viele Artefakte übernommen, was sehr interessant ist.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/3fc19bebc0f08301c261b88c556c709a.jpg}\\[2pt]{\small\color{darktext!70} Originalfoto: Soviet-inspiriertes Café Badezimmer}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/7e077b58c543b81770e4c4808bce59bb.png}\\[2pt]{\small\color{darktext!70} Generierte 3D-Welt basierend auf dem Foto}
\end{minipage}
\end{figure}
```

#### Welt 2: Kepler in Spiegelscherben

Für die zweite Welt habe ich ein Foto von Kepler verwendet – ohne zusätzliche Prompt-Beschreibung, nur das Bild als Input.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/cde09f49816153b36b870a7b7e935bce.jpg}\\[2pt]{\small\color{darktext!70} Originalfoto: Kepler}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/b33c5185c43cbde36c24935907d5c8fc.png}\\[2pt]{\small\color{darktext!70} Generierte 3D-Welt: Kepler in Spiegelscherben}
\end{minipage}
\end{figure}
```

Das Ergebnis ist ein bisschen Uncanny Valley – Kepler steht da mit seltsamen Proportionen. Interessant ist, dass der Hintergrund einfach schwarz ist, wo das Modell nichts sehen kann. Das passt aber zur düsteren, fragmentierten Welt. In der Unschärfe als CG-Hintergrund könnte das funktionieren.

#### Welt 3: Unterwasser-Pool

Hier habe ich ein Pinterest-Bild aus dem Board für den Song verwendet und eine kurze Prompt-Beschreibung hinzugefügt.

**Prompt:** "Underwater pool view with inflatable rings floating above. Summer Feelings Refractions."

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/c3c49f49c472deef21161f2b9ed77cd6.jpg}\\[2pt]{\small\color{darktext!70} Originalfoto: Pool}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/340cb65e0d98e558a1cf7aa5a085fb57.png}\\[2pt]{\small\color{darktext!70} Generierte 3D-Welt: Unterwasser-Pool}
\end{minipage}
\end{figure}
```

Mir fällt auf, dass sich das Modell sehr nah am Input-Bild orientiert. Das ist nicht unbedingt schlecht, aber es limitiert die kreative Interpretation. Für abstraktere Ergebnisse brauche ich einen anderen Ansatz.

#### Welt 4: Botanischer Garten (Claude-generierte Prompt)

Da ich als Free-User nur ein einziges Bild hochladen kann, habe ich einen Workaround versucht: Mehrere Pinterest-Bilder in den Claude-Chat hochgeladen und mir eine Marble-Prompt generieren lassen.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/a9a9fe3afbf560425fea7c87772a0ba9.jpg}\\[2pt]{\small\color{darktext!70} Claude-Input: Pinterest-Bilder 1}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/6f951e096bb71849bc027b782a7e7f59.jpg}\\[2pt]{\small\color{darktext!70} Claude-Input: Pinterest-Bilder 2}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/65c5e75f269ab3f231f0ddd31e8c0e1d.jpg}\\[2pt]{\small\color{darktext!70} Claude-Input: Pinterest-Bilder 3}
\end{minipage}
\end{figure}
```
**Claude-generierte Prompt:** "Translucent glass botanical sculpture, iridescent resin figure with embedded flowers, ethereal bioluminescent flora, soft pastel gradients of lavender, mint, coral and gold, delicate organic forms suspended in crystalline transparency, surreal dreamlike atmosphere, intricate internal structures visible through glossy surfaces, dewdrops and light refractions, alien botanical specimens with thorny stems and luminous petals, macro photography aesthetic, soft bokeh backgrounds, hyperdetailed textures mixing smooth glass with organic growth, otherworldly garden specimens caught between organic and synthetic, studio lighting with rim light highlights, volumetric light rays, 8k cinematic quality"

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/0b5376968691affa6849c19ca19e84a2.png}\\[2pt]{\small\color{darktext!70} Generierte 3D-Welt: Botanischer Garten 1}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/8f1f22d8f8cc0adc0290bde5a183206d.png}\\[2pt]{\small\color{darktext!70} Generierte 3D-Welt: Botanischer Garten 2}
\end{minipage}
\end{figure}
```

Dieser Ansatz – Vibes statt konkretes Referenzbild – hat interessantere Ergebnisse gebracht. Die Welt fühlt sich weniger wie eine Rekonstruktion und mehr wie eine Interpretation an.

---

#### Fazit zu den Experimenten

Marble orientiert sich stark am Input-Bild. Für kreativere Ergebnisse funktioniert es besser, nur Text-Prompts zu verwenden oder abstrakte Inputs. Der Workaround über Claude zur Prompt-Generierung aus mehreren Bildern ist eine interessante Methode, um die Einbild-Limitierung zu umgehen.

#### Die Technologie dahinter

Marble ist das erste kommerzielle Produkt von [World Labs](https://www.worldlabs.ai), einem Startup gegründet von der KI-Pionierin Fei-Fei Li (bekannt für ImageNet). Im Kern ist Marble ein sogenanntes "World Model" – ein KI-System, das aus Text, Bildern oder Videos persistente 3D-Umgebungen generiert.

**Was Marble von anderen Tools unterscheidet:**
- **Persistente Welten:** Anders als Systeme wie Decart oder Odyssey, die Welten on-the-fly generieren, erstellt Marble herunterladbare 3D-Umgebungen
- **Multimodaler Input:** Text, Bilder, Videos oder grobe 3D-Layouts
- **Exportformate:** Gaussian Splats, Meshes oder Videos
- **VR-ready:** Native Unterstützung für Vision Pro und Quest 3

**Chisel Editor:** Ein experimenteller 3D-Editor, der strukturierte Layouts (Wände, Boxen, Ebenen) mit Style-Prompts kombiniert – ähnlich wie HTML die Struktur und CSS den Stil einer Website definiert.

**Timeline:**
- September 2025: Limited Beta
- 12. November 2025: Public Release mit Freemium-Modell

*Quellen: [World Labs Blog](https://www.worldlabs.ai/blog/marble-world-model), [TechCrunch](https://techcrunch.com/2025/11/12/fei-fei-lis-world-labs-speeds-up-the-world-model-race-with-marble-its-first-commercial-product/)*

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du verwendet?
\end{claudemsg}
\begin{usermsg}
Marble (marble.worldlabs.ai).
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Ich wollte zum einen das Tool testen, ich hatte es gerade gesehen und da die Gesichter ja nicht funktioniert haben, also Kepler als Charakter nicht funktioniert hat im Comp-V-Wire-Workflow, hatte mich interessiert, ob wir vielleicht Hintergrundwelten generieren könnten.
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? \emph{(Workflow)}
\end{claudemsg}
\begin{usermsg}
Ich habe hier mit Marble erstmal experimentiert. Ich kannte bereits Gaussian Splats als Konzept und ich fand es interessant, wie schnell das jetzt ging, aus einem einzigen Bild einen übereinstimmenden Splat zu erzeugen.\par
Ich war aber jetzt nicht so überzeugt davon, dass ich es sicher verwenden werde.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle erlebt? \emph{(Autorschaft)}
\end{claudemsg}
\begin{usermsg}
Einfach nur jemand, der experimentiert und ein Produkt ausprobiert.
\end{usermsg}
\begin{claudemsg}
Wie hat die KI-Ausgabe deinen Prozess beeinflusst? \emph{(Iteration)}
\end{claudemsg}
\begin{usermsg}
Das Ausprobieren machte Spaß, es ging schnell, und das könnte für die Zukunft interessant sein.\par
Aber mich interessieren dann eher die generativen Möglichkeiten. Also, wirklich, sich komplette Welten ausdenken anstatt existierende Welten 3D zu machen. Obwohl ich, wenn ich jetzt darüber nachdenke, mir gar nicht mehr so sicher bin. Es kann schon interessante Use Cases haben...
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen? \emph{(Scheitern)}
\end{claudemsg}
\begin{usermsg}
Nicht unbedingt, außer dass das freiere, letzte Ergebnis nicht hundertprozentig überzeugend ist, aber schon mal interessant.\par
Und dass, als ich Personen drin hatte mit Spiegelungen und so, dass die Personen gruselig und Uncanny-Valley-mäßig verzerrt waren.\par
Aber ich hatte jetzt gar keine Ansprüche an das Tool und würde Input-Bilder ohne Personen oder komplexe Spiegelungen nehmen das nächste Mal.
\end{usermsg}
\begin{claudemsg}
Hat sich eine eigene visuelle Sprache entwickelt? \emph{(Ästhetik)}
\end{claudemsg}
\begin{usermsg}
Noch nicht, also ich habe nur im Hinterkopf ein neues Tool, das ich verwenden könnte.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Im Eintrag steht, dass das System das Spiegelbild als Spiegel erkannt und die gespiegelten Personen in den 3D-Raum gebracht hat – faszinierend und gruselig zugleich. Könntest du dir vorstellen, solche 'Fehler' absichtlich als ästhetisches Mittel zu nutzen?
\end{claudemsg}
\begin{usermsg}
Jetzt, wo du es sagst, ja, also man könnte tatsächlich wirklich damit visuell spielen. Ich weiß nur nicht, wie einfach es ist, das wirklich jetzt im 3D-Raum umzusetzen.\par
Klar, wenn ich ein reales Bild von ihm nehme und dann ein Gaussian-Splat erzeuge, könnte das interessant sein. Aber ich bin ja eher in der 3D/Computergrafik-Welt verankert und arbeite nicht viel mit Fotos.\par
Aber es ist auf jeden Fall ein cooler Ansatz. Wobei, da fällt mir gerade ein, ich könnte auch 3D gerenderte Bilder in ein Gaussian Splat Generator eingeben.
\end{usermsg}
```

\newpage

### Journal-Eintrag 4: Website mit Cursor aufsetzen (19. November 2025)

Für mein Artistic Research-Projekt "Everything Machine (Kepler)" brauchte ich eine Website, die sowohl den wissenschaftlichen Anspruch erfüllt als auch die künstlerische Identität von Kepler widerspiegelt. Statt mit einem fertigen CMS zu arbeiten, habe ich mich entschieden, die Seite von Grund auf mit [Cursor](https://cursor.sh) zu bauen – einem AI-gestützten Code-Editor, der mir dabei hilft, schneller zu entwickeln und gleichzeitig den Code sauber zu halten.

#### Technical Setup: Cursor und GitHub

Cursor ist im Grunde ein Fork von VS Code, der mit KI-Features erweitert wurde. Man kann direkt im Editor mit der KI chatten, Code generieren lassen oder bestehenden Code refactoren. Für mich war das perfekt, weil ich so nicht ständig zwischen Browser und Dokumentation wechseln musste – die KI kennt den Kontext meines Projekts und kann direkt helfen.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/6f4dbdbae82aec3155e053d33a69551f.png}
\caption{Cursor Oberfläche. Links die Liste der Dateien im GitHub Repository, in der Mitte eine Datei mit Changes, die die KI gemacht hat (die accepted werden können – in diesem Fall ein Prompt um das Layout zu fixen), rechts der Agent View, der ein Modell (in diesem Fall GPT 5.1 Codex) nutzt, um Prompts umzusetzen.}
\end{figure}
```
Das Projekt ist komplett in Git versioniert. Jede größere Änderung wurde als Commit festgehalten, was es mir ermöglicht, den Entwicklungsprozess nachzuvollziehen. Die Struktur ist simpel: `index.html` für die Hauptseite, `scripts.js` für die Logik, `style.css` für das Styling, und ein `journal/` Ordner für alle Markdown-Einträge. Keine komplexen Build-Tools, keine Dependencies außer Tailwind CSS über CDN – alles läuft direkt im Browser.

#### Design: Research Catalogue inspiriert, aber dunkel

Als Inspiration diente mir die [Research Catalogue](https://www.researchcatalogue.net/) – eine Plattform für Artistic Research, die ich während meines Studiums kennengelernt habe. Die hat diesen cleanen, wissenschaftlichen Look mit klarer Navigation und guter Lesbarkeit. Ich habe mir das Konzept geliehen, aber es für Kepler angepasst.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/58b21aeac66392aec4c62d430da85c08.png}
\caption{Research Catalogue Layout-Beispiel. Ein cooler Artikel auf researchcatalogue.net, der das Layout demonstriert – cleanes Design mit klarer Navigation und guter Lesbarkeit.}
\end{figure}
```
Ein besonders gutes Beispiel für das Research Catalogue Design ist dieser [Design Phenomenographies for Industrial Wastelands](https://www.researchcatalogue.net/view/2829981/3316740), der zeigt, wie multimediale Inhalte in einem wissenschaftlichen Kontext präsentiert werden können.

Statt hellem Hintergrund gibt es einen dunklen Theme (#111), der besser zu Keplers digitaler Identität passt. Die Navigation funktioniert ähnlich wie bei Research Catalogue – ein Dropdown-Menü oben links mit den Hauptsektionen. Links werden als "Pills" dargestellt (abgerundete Buttons), was moderner wirkt als normale Textlinks.

Das Besondere sind die Pixel-Headings. Kepler hat diese Voxel-Ästhetik, und um das visuell zu transportieren, verwende ich die "Press Start 2P" Font von Google Fonts für alle Überschriften. Das gibt der Seite sofort diesen retro-digitalen Vibe, ohne dass es zu verspielt wirkt. Die Hauptüberschriften im Journal haben sogar einen animierten Gradient-Effekt, der von grün zu weiß übergeht – eine Referenz an die Matrix-Ästhetik, die zu Keplers digitaler Existenz passt.

#### Musikplayer: Medien-Enhancement im Header

Ein wichtiger Teil des Projekts ist die Musik von Kepler. Statt die Songs nur als statische Links einzubinden, habe ich einen Mini-Player in die Navigation integriert. Der bleibt immer sichtbar, auch wenn man durch die Seiten navigiert, und spielt die Songs aus einer Playlist ab.

Technisch funktioniert das so: In `scripts.js` gibt es ein `playlist` Array, in dem alle Songs mit Metadaten (Titel, Album, Cover, Spotify-Link) gespeichert sind. Der Player nutzt das native HTML5 `<audio>` Element und zeigt den aktuellen Track in einem Marquee-Text an. Über einen Playlist-Button kann man alle verfügbaren Songs sehen und direkt wechseln.

Die Implementierung war relativ simpel – keine externe Library nötig. Der Player lädt die Metadaten (Dauer) asynchron, damit die Seite schnell bleibt. Die Playlist wird dynamisch generiert, sodass ich einfach neue Songs zum Array hinzufügen kann, ohne HTML ändern zu müssen.

#### Titelseite: Abstract als Basis

Die erste Seite ("Projektübersicht") basiert direkt auf meinem Abstract. Der Text steht prominent oben in einem abgesetzten Container, gefolgt von Einleitung, Forschungsfragen und Methodologie. Das Layout ist zentriert und auf Lesbarkeit optimiert – max-width von 4xl, damit die Zeilenlänge nicht zu lang wird.

Die Struktur ist bewusst einfach gehalten: keine komplexen Animationen, keine überladenen Effekte. Der Fokus liegt auf dem Inhalt, was für eine wissenschaftliche Arbeit wichtig ist. Die Bilder sind in einem Grid angeordnet, mit Fallback-Placeholders falls Dateien fehlen.

#### Logbuch: Pixel-Effekt und horizontale Timeline

Das Herzstück der Website ist das Prozess-Logbuch. Hier werden alle Journal-Einträge aus dem `journal/` Ordner automatisch geladen und angezeigt. Jeder Eintrag ist eine Markdown-Datei, die beim Laden geparst und in ein Grid-Layout umgewandelt wird.

Die Timeline oben ist horizontal scrollbar und zeigt alle verfügbaren Einträge mit Datum und Titel. Klickt man auf einen Eintrag, wird er im Hauptbereich angezeigt. Die Einträge selbst werden in "Bubbles" dargestellt – jeder Abschnitt (überschrieben mit ## oder ###) wird zu einer eigenen Karte im Grid.

Der Pixel-Effekt kommt beim Titel zum Tragen: Wenn ein Eintrag geladen wird, erscheint der Titel mit einem Typing-Effekt, Buchstabe für Buchstabe, als würde jemand tippen. Dazu gibt es einen blinkenden Cursor. Die Font ist "Press Start 2P", und der Titel hat einen animierten Gradient, der von grün zu weiß übergeht.

Technisch wird das so umgesetzt: Der Markdown-Parser (`parseMarkdown` Funktion) teilt jeden Eintrag in Abschnitte auf. Überschriften werden zu Bubble-Titeln, der Rest wird als HTML formatiert. Bilder, Links und spezielle Komponenten (wie die WORLD_INFO Dropdowns für Marble-Welten) werden dabei berücksichtigt. Das Grid passt sich automatisch an die Bildschirmgröße an – 3 Spalten auf Desktop, 2 auf Tablet, 1 auf Mobile.

#### Git-Versionierung und Projektstruktur

Jeder größere Entwicklungsschritt wurde in Git committet. Die Commits zeigen den Prozess: Zuerst die Grundstruktur, dann der Player, dann das Journal-System, dann die Timeline, dann die Bubble-Layouts. Das macht es einfach, zu sehen, wie sich die Website entwickelt hat.

Die Projektstruktur ist bewusst flach gehalten:
- `index.html` – Haupt-HTML
- `scripts.js` – Alle JavaScript-Logik (Player, Journal-Loader, Navigation)
- `style.css` – Alle Styles (Dark Theme, Grid-Layouts, Animationen)
- `journal/*.md` – Markdown-Einträge, die automatisch geladen werden
- `assets/` – Bilder, Audio, Journal-Assets

Keine Node-Modules, keine Build-Steps – alles läuft direkt im Browser. Das macht die Entwicklung schnell und das Deployment einfach. Für lokales Testen reicht ein einfacher HTTP-Server (z.B. `python -m http.server 8001`).

#### Fazit

Die Website ist jetzt in Version 1.0 – Player funktioniert, Journal lädt automatisch, Design ist konsistent. Cursor hat den Prozess deutlich beschleunigt, besonders bei repetitiven Aufgaben wie dem Markdown-Parsing oder dem Grid-Layout. Die Git-Versionierung gibt mir Sicherheit, dass ich jederzeit zurückkehren kann, wenn etwas schiefgeht.

Was mir besonders geholfen hat: Ich habe bereits Erfahrung mit Web-Entwicklung, und das macht einen großen Unterschied, wenn man mit KI programmiert. Man kann die Vorschläge der KI besser einschätzen, weiß, was funktioniert und was nicht, und kann gezielt nachfragen oder anpassen. Cursor ist ein mächtiges Tool, aber es ersetzt nicht das Verständnis für die Grundlagen – es beschleunigt den Workflow enorm, wenn man weiß, was man will. Die Basis steht, und das war das Ziel.

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du verwendet?
\end{claudemsg}
\begin{usermsg}
Cursor, Version Control via Github, und größtenteils die Modelle von Anthropic.
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Ich wollte sowohl die Website erstellen als auch Cursor ausprobieren, da es sich interessant für mich anhörte und ich bereits Webentwicklungserfahrung habe, aber nur sehr limitiert und es würde sehr lange dauern, wenn ich es ganz alleine mache.
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? \emph{(Workflow)}
\end{claudemsg}
\begin{usermsg}
Oh mein Gott, das war einfach sehr krass! Das hat das Aufsetzen der Website einfach tausendfach schneller gemacht.\par
Also, hier war der größte Unterschied zu merken, wo ich konnte mein bestehendes Wissen einbringen. Aber musste bei Kleinigkeiten nicht selber googeln und bug fixen, sondern Cursor. Die KI-Modelle haben das für mich erledigt. Ich konnte schnell experimentieren und ich hatte zwar Probleme beim Debugging, aber ich wäre gar nicht so weit gekommen ohne Cursor und Claude. Ich konnte quasi meine kreative Vision umsetzen.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle erlebt? \emph{(Autorschaft)}
\end{claudemsg}
\begin{usermsg}
I felt like a manager with the overall vision asking a junior developer to code for me. At some point, I felt like I was collaborating a bit when it couldn't figure out where it was going wrong. And I was reviewing a lot what I was doing. At the same time, I was learning because when I was doing things I couldn't have done myself, I asked it, or I could see its thinking process too. So I also learned a lot.
\end{usermsg}
\begin{claudemsg}
Wie hat die schnelle KI-Ausgabe deinen Prozess beeinflusst? \emph{(Iteration)}
\end{claudemsg}
\begin{usermsg}
Die KI-Ausgabe war sofort da. Code, Layouts, Fixes. Das hat meinen Workflow massiv beschleunigt, weil ich nicht mehr googeln und Stack Overflow durchforsten musste. Aber es hat auch dazu geführt, dass ich manchmal zu schnell akzeptiert habe, ohne den Code richtig zu verstehen. Das Tempo verleitet zur Oberflächlichkeit.
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen? \emph{(Scheitern)}
\end{claudemsg}
\begin{usermsg}
Also, wir kamen ab und zu in Debugging-Schleifen. Das war schon sehr nervig, aber ich habe dann besser gelernt, wie man die KI anleiten kann. Zum Beispiel, dass man öfter den Chat erneuern sollte, damit umso mehr Informationen im Kontext sind, desto schlechter die KI funktioniert. Es wurden auch falsche Ansätze gewählt, welche ich dann ablehnen musste bzw. zurückrudern musste.
\end{usermsg}
\begin{claudemsg}
Hat sich eine eigene visuelle Sprache entwickelt? \emph{(Ästhetik)}
\end{claudemsg}
\begin{usermsg}
Ja, ein bisschen Kepler-DNA und der Matrix-Look. Ich mag dunkle Websites, und ich hatte mich ja vom Research Catalogue inspirieren lassen. Da waren die Artikel, die mich inspiriert haben, aber weiß, und ich wollte einen dunklen Look für dieses Projekt.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Du hast geschrieben, dass deine bestehende Web-Erfahrung entscheidend war, um KI-Vorschläge einzuschätzen. Glaubst du, jemand ohne diese Erfahrung hätte ein ähnliches Ergebnis erreichen können?
\end{claudemsg}
\begin{usermsg}
Also ich glaube, es hängt sehr stark von der Person ab. Ich glaube, Leute, die gar keinen Hintergrund haben und auch nicht so viel Geduld, würden schnell an ihre Grenzen kommen. Also, es ist nicht so, dass man einfach alles an die KI geben kann. Garbage in, garbage out. Man muss schon ein bisschen mitlernen, obwohl es schon Ergebnisse gibt, die mit relativ viel wenig Arbeit gut sein können. Ich denke, wenn jemand einfach nur sagt: "Hey, kannst du mir einen Taschenrechner programmieren? Vielleicht in einer bestimmten Farbe, die mir gefällt?" – dann würde das schnell funktionieren, einfach nur weil es so viel vorhandener Code für das Programmieren von Taschenrechnern gibt.\par
Aber umso spezifischer die Anforderungen werden, umso länger der Chat und umso größer die codebase, desto schwieriger wird es das ganze für einen Laien zu managen.
\end{usermsg}
```

\newpage

### Journal-Eintrag 5: KI wirklich verstehen -- Teil 1 (11. Januar 2026)

*Ich wollte Künstliche Intelligenz besser für mich selbst verstehen, auch außerhalb des Artistic Research Contexts, bevor ich dort weiter eintauchte. Ich wollte nicht nur die Tools benutzen, die aufkommen, sondern wirklich verstehen, was dort gerade vor sich geht.*

*Eigentlich wollte ich auf der Autofahrt einen Podcast hören. Aber die meisten, die ich gefunden habe, waren für sehr einfache Anwendungsfälle – wie können normale Menschen ChatGPT nutzen, um E-Mails zu schreiben. Ich wollte in die tieferen Themen rein. Ich hatte das Gefühl, dass manche Leute viel weiter sind als ich, und ich wollte genau die Lücken aufholen, wo ich durchfalle.*

*Dann dachte ich: Perfekt wäre eigentlich KI selbst, weil die ja immer verfügbar ist. Also Freisprechanlage an, Claude im Chat-Interface, das meine Stimme in Text transkribierte. Zugegebenermaßen etwas gefährlich am Anfang – das System stockte manchmal, ich musste Sätze wiederholen, war abgelenkt. Aber dann hatten wir uns eingespielt, und es wurde zu einem echten Gespräch über 30 Minuten Fahrtzeit.*

---

```{=latex}
\begin{usermsg}
Ich will mehr über KI lernen. Ich will das wirklich tiefer verstehen.
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Vielleicht kannst du ein Gespräch mit mir führen und mein Level einschätzen, und mir dann ein Level höher erklären?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Klar! Erzähl mir erstmal, was du schon weißt und womit du arbeitest.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Also ich nutze Claude Code gerade, hab ein paar GitHub-Repos für Research, schreibe meine Masterarbeit, mache generelle Recherche zu PhD-Themen...
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich hab Web-Development-Erfahrung und bin generell im Computer Graphics Bereich. Also breites aber vielleicht nicht super tiefes Wissen.
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich will die Grundlagen von KI und KI-Forschung verstehen. Wie fängt der Weg von der Mathematik zur KI an?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Gib mir ruhig lange Antworten. Ich kann gerade nicht lesen, also erkläre es ausführlich.
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Mathematik ist die Sprache, die KI spricht, weil Computer nur Zahlen verstehen. Wenn ein Mensch ein Foto sieht, verarbeitet das Gehirn Farben, Formen und Muster sofort – aber ein Computer sieht dasselbe Foto als Millionen von Zahlen, die Pixelwerte darstellen. Jede KI beginnt damit, reale Informationen in mathematische Repräsentationen umzuwandeln, die Computer manipulieren können.\par
    Der Weg von einfacher Mathematik zu KI beginnt mit Statistik und Wahrscheinlichkeit. Wenn Menschen Entscheidungen treffen, berechnen sie unbewusst Chancen basierend auf vergangenen Erfahrungen. Wer dunkle Wolken sieht, greift wahrscheinlich zum Regenschirm, weil er gelernt hat, dass dunkle Wolken oft Regen bedeuten.\par
    Frühe KI-Forscher versuchten, genau das nachzuahmen: Systeme zu erschaffen, die aus Beispielen lernen, statt starren Regeln zu folgen. Statt einem Computer jeden möglichen Schachzug einzuprogrammieren, erkannten sie, dass man ihm Tausende von Schachpartien zeigen und ihn selbst Gewinnmuster entdecken lassen konnte.\par
    Dafür brauchte es drei mathematische Durchbrüche: Erstens lineare Algebra, um massive Datenmengen gleichzeitig zu verarbeiten. Zweitens Analysis, um die Schätzungen des Systems zu optimieren und zu verbessern. Drittens Wahrscheinlichkeitstheorie, um mit Unsicherheit umzugehen und Vorhersagen auch bei unvollständigen Informationen zu treffen.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Ja. Das wurde ja im letzten Jahrhundert ein Ding... Das basiert auf Mathe.
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Hilf mir, den Weg von 1920 bis 2020 zu verstehen. Wann hat KI eigentlich angefangen? Wo wurde geforscht? Wie kam es zu Durchbrüchen?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Das formale Feld der künstlichen Intelligenz wurde auf der Dartmouth-Konferenz im Sommer 1956 geboren. Vier brillante Forscher überzeugten die Rockefeller Foundation, einen ambitionierten achtwöchigen Workshop zu finanzieren. Ihr Antrag war unglaublich kühn – sie glaubten, dass "jeder Aspekt des Lernens oder jede andere Eigenschaft der Intelligenz im Prinzip so präzise beschrieben werden kann, dass eine Maschine sie simulieren kann".
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Ja, die Prägung des Begriffs. Erzähl mir mehr.
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
John McCarthy von Dartmouth erfand buchstäblich den Begriff "Artificial Intelligence" für diesen Workshop, weil sie einen einprägsamen Namen für ihren Förderantrag brauchten. Vorher nannte man das "thinking machines" oder "electronic brains" – zu sehr nach Science-Fiction.\par
    Die ursprüngliche Gruppe umfasste McCarthy, der später die Programmiersprache LISP erfand; Marvin Minsky von Harvard, der zum Vater der KI am MIT wurde; Claude Shannon von Bell Labs, der die Informationstheorie erschuf und damit digitale Kommunikation erfand; und Nathaniel Rochester von IBM, der einige der ersten elektronischen Computer entwarf.\par
    Sie luden zehn Teilnehmer ein, darunter Allen Newell und Herbert Simon von Carnegie Mellon, die ihren "Logic Theorist" mitbrachten – ein Programm, das tatsächlich mathematische Theoreme beweisen konnte. Das war das erste echte KI-Programm, das funktionierte.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Oh, das ist super interessant. Das will ich in meiner Masterarbeit haben...
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Kannst du das noch mehr erklären? Das mit Claude Shannon?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Claude Shannons Informationstheorie von 1948 ist fundamental für alles in der KI. Shannon fand heraus, wie man Information mathematisch in Bits messen kann. Er bewies, dass jede Information – ob Text, Bilder oder Ton – in Binärcode umgewandelt und mit perfekter Genauigkeit übertragen werden kann, wenn genug Fehlerkorrektur hinzugefügt wird.\par
    Das ist gewaltig, weil es bedeutet, dass alles menschliche Wissen digitalisiert und von Maschinen verarbeitet werden kann. Shannons Arbeit bei Bell Labs ermöglicht direkt Computer, das Internet und jede digitale Technologie, die wir heute nutzen.\par
    Information hat eine mathematische Struktur. Ein Münzwurf liefert genau ein Bit Information, weil er Unsicherheit zwischen zwei gleich wahrscheinlichen Ergebnissen auflöst. Eine Nachricht, die sagt "die Sonne wird morgen aufgehen", enthält fast keine Information, weil sie nahezu sicher ist.\par
    Dieser mathematische Rahmen ermöglicht es Computern, Bedeutung zu verarbeiten – nicht nur Zahlen. Wenn jemand einen Satz tippt, reduziert jeder Buchstabe die Unsicherheit darüber, was als nächstes kommt, basierend auf Mustern in der Sprache. Moderne KI-Systeme nutzen dieses Prinzip, um das nächste Wort in einer Sequenz vorherzusagen.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Okay. Super Erklärung. Kannst du da noch ein bisschen tiefer reingehen?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Und wird das heute noch benutzt, die symbolischen Repräsentationen? Das sind ja Weights und Biases jetzt, oder?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Symbolische KI und moderne neuronale Netze mit Gewichten und Bias sind fundamental unterschiedliche Ansätze zur Wissensrepräsentation.\par
    Der symbolische Ansatz versucht, Wissen explizit in menschenlesbaren Regeln und Logik zu erfassen. Wenn der Logic Theorist weiß "Alle Menschen sind sterblich", existiert diese Regel buchstäblich im Code als Symbole, die gelesen und verstanden werden können.\par
    Aber Gewichte und Bias in neuronalen Netzen repräsentieren Wissen auf eine völlig verteilte, implizite Weise. Ein neuronales Netz, das etwas über Sterblichkeit weiß, hat diese Regel nirgendwo geschrieben. Stattdessen entsteht dieses Wissen aus Millionen numerischer Gewichte, die statistische Muster über alle Trainingsdaten kodieren.\par
    Der Schlüsselunterschied: Symbolische KI speichert Wissen so, wie Menschen über Wissen denken – mit klaren Konzepten und logischen Beziehungen. Neuronale Netze speichern Wissen so, wie Gehirne tatsächlich funktionieren könnten – als Verbindungsstärken zwischen einfachen Verarbeitungseinheiten.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Okay. Wie realistisch ist es für mich, das zu verstehen?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich glaub nicht, dass es so sehr vom Anwendungsfall abhängt... Ich meine, Large Language Models sind ja was anderes als, sagen wir, Blur-Filter in Photoshop, oder?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich meine, wir hatten ja schon Noise Reduction und Color Grading. Wieso fühlt sich jetzt alles gleichzeitig an?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Es gibt tatsächlich ein Spektrum von einfachen deterministischen Filtern bis zu moderner KI. Traditionelle Photoshop-Filter wenden feste mathematische Operationen an. Ein Blur-Filter konvolviert jedes Pixel mit seinen Nachbarn mit demselben Kernel. Deterministisch und vorhersagbar.\par
    Die erste Evolution waren adaptive Algorithmen. Noise Reduction in den frühen 2000ern begann, statistische Analyse zu nutzen, um Signal von Rauschen zu unterscheiden. Statt überall denselben Filter anzuwenden, analysierten diese Tools lokale Bildeigenschaften und passten ihr Verhalten an.\par
    Dann kam Machine Learning um 2010. Statt von Hand zu kodieren, wie man Kanten erkennt, trainierten Forscher Algorithmen auf Tausenden von gelabelten Beispielen. Das waren noch keine neuronalen Netze, sondern statistische Mustererkennung.\par
    Was alles veränderte, war Skalierung. GPUs machten es möglich, viel größere Netze auf viel mehr Daten zu trainieren. Dieselben mathematischen Prinzipien, die frühe Computer Vision antrieben, wurden plötzlich mächtig genug, um komplexe Muster in Bildern, Text und anderen Domänen zu erkennen.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Und ja. Und dann, je nachdem... GPT-2 kam raus...
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
GPT-2 war der Moment, in dem Forscher realisierten, dass Skalierung qualitativ neue Fähigkeiten erschafft – nicht nur bessere Leistung bei denselben Aufgaben.\par
    Vor GPT-2 dachte jeder, dass spezifische KI-Systeme für jede Aufgabe trainiert werden müssen. Will man Übersetzung? Trainiere ein Übersetzungsmodell. Will man Zusammenfassung? Trainiere ein Zusammenfassungsmodell. GPT-2 zeigte, dass ein genügend großes Sprachmodell, das einfach trainiert wurde, das nächste Wort vorherzusagen, plötzlich all diese Aufgaben erledigen konnte, ohne jemals explizit dafür trainiert worden zu sein.\par
    Das schockierte die Forscher. Sie erwarteten, dass größere Modelle besser darin werden, Text vorherzusagen, aber sie erwarteten nicht, dass sie spontan Reasoning-Fähigkeiten entwickeln würden, grundlegende Arithmetik oder die Fähigkeit, Code zu schreiben. Diese Fähigkeiten entstanden rein durch Skalierung.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Ja. Jetzt ist es ja nicht mehr nur das Training der Modelle...
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Also ich nutze KI ja nicht nur für Text. Auch für Bilder, 3D, Video...
\end{usermsg}
```

```{=latex}
\begin{usermsg}
MCP, und jetzt Skills versus Agents und so. Können wir noch eine Sekunde auf dieser Ebene bleiben?
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Die zentrale Einsicht ist, dass Transformer – die Architektur, die Sprachmodelle antreibt – jeden Datentyp verarbeiten können, sobald er in Tokens umgewandelt ist. Text wird in Wort-Teile tokenisiert, Bilder werden in Patches unterteilt, Audio wird zu Spektrogrammen. Einmal alles in Tokens, kann dieselbe mathematische Maschinerie alles verarbeiten.\par
    Retrieval Augmented Generation (RAG), wie es Tools wie Claude Code nutzen, löst das Problem des Wissens-Cutoffs. Statt zu versuchen, alles während des Trainings zu memorieren, durchsuchen RAG-Systeme externe Datenbanken in Echtzeit und injizieren diese Information in den Kontext des Modells.\par
    MCP – Model Context Protocol – ist im Grunde ein standardisierter Weg für KI-Modelle, sich mit externen Tools und Datenquellen zu verbinden. Statt dass jede Firma eigene Integrationen baut, schafft MCP eine universelle Schnittstelle. Wie USB für KI.\par
    Skills sind spezifische Fähigkeiten, die eine KI ausführen kann – wie "Spreadsheets analysieren" oder "Code generieren". Sie sind reaktive Tools. Agents sind autonome Systeme, die planen, mehrere Schritte ausführen und selbstständig auf Ziele hinarbeiten können.\par
    Der vereinigende Faden sind Attention-Mechanismen – die mathematische Operation, die Transformern erlaubt, sich auf relevante Informationen zu fokussieren, unabhängig vom Datentyp. Deshalb explodieren KI-Fähigkeiten gleichzeitig über alle Domänen – dieselbe Kerntechnologie funktioniert überall, sobald das Tokenisierungsproblem gelöst ist.
\end{claudemsg}
```

---

#### Wie lerne ich das wirklich?

*Gegen Ende der Fahrt wurde mir klar: Ich hatte jetzt all diese Informationen gehört – aber habe ich sie wirklich gelernt?*

```{=latex}
\begin{usermsg}
Okay, wenn ich dieses Thema wirklich tief verstehen will, und auch die Brücke schlagen will zu den Technologien, die ich benutze – Cursor, Claude Code für Research und Coding, ComfyUI, Houdini, Unreal Engine – was wäre der beste Weg dahin?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Sollte ich den Text selbst schreiben? Auch wenn du ihn generiert hast? Es gibt ja diese Studien, dass das Verständnis nicht so tief ist, wenn Essays mit KI geschrieben werden. Aber ich brauche dieses Fundament, um weiterzumachen.
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Andererseits habe ich das Gefühl, dass ich Wissen schrittweise aufbaue. Manchmal wollte ich Computer Graphics wirklich tief verstehen. Hab reingeschaut und dann aufgehört. Aber ich glaube, ich habe danach ein höheres Verständnis. Oder bilde ich mir das nur ein?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Zum Beispiel Houdini lernen – zum vierten oder fünften Mal. Ich mache langsam Fortschritte. Wie lernen menschliche Gehirne eigentlich?
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich hatte meinen ersten Computer mit 7. Einfache Spiele, Kekse dekorieren. Dann hab ich viel gezeichnet und war im Internet, dann hab ich eine Pokemon-Website gemacht, indem ich HTML und CSS von jemand anderem kopiert habe. In der Schule hatte ich Programmieren, ganz basic. Dann Innenarchitektur studiert mit 3ds Max und am Ende Unreal Engine. Das ist viel. Ein langer Weg, um all dieses spezialisierte Wissen zu sammeln.
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Du stellst die richtige Frage, und die ehrliche Antwort ist kompliziert.\par
    Die Forschung, die du meinst, ist real – wenn du etwas selbst schreibst, wirst du gezwungen, dich mit Lücken in deinem Verständnis auseinanderzusetzen. Lesen oder Zuhören fühlt sich wie Lernen an, weil die Information glatt fließt, aber diese Flüssigkeit ist trügerisch. Du kannst bei einer Erklärung von Backpropagation nicken, ohne sie fünf Minuten später selbst erklären zu können. Das nennt man die Fluency Illusion.\par
    Aber hier ist die Nuance: Es gibt verschiedene Arten von Wissen. Deklaratives Wissen ist "wissen dass" – Fakten, Konzepte, Geschichte. Prozedurales Wissen ist "wissen wie" – Dinge tatsächlich tun. Dein Weg von Kekse-Dekorieren über Pokemon-HTML zu 3ds Max zu Unreal ist prozedurales Wissen, das sich ansammelt. Das ist real und bleibt hängen auf eine Weise, die Lesen niemals erreicht.\par
    Dein Muster, Houdini aufzugreifen, fallen zu lassen, Monate später zurückzukommen und etwas besser zu sein – so funktioniert tiefes Lernen tatsächlich. Dein Gehirn konsolidiert während der Pausen. Schlaf, Zeit weg, angrenzende Dinge tun – das sind keine Misserfolge, das ist Teil des Prozesses. Der vierte Versuch fängt nicht von vorne an, er baut auf neuronalen Bahnen auf, die sich während der Versuche eins bis drei gebildet haben.\par
    Um KI-Verständnis mit deinen tatsächlichen Tools zu verbinden, würde ich vorschlagen: Schreib meinen Text nicht um. Stattdessen: Nimm einen Workflow, den du schon machst – vielleicht Depth-Map-Extraktion in ComfyUI – und verfolge die KI-Pipeline rückwärts. Welches Modell wird verwendet? Worauf wurde es trainiert? Was passiert mathematisch? Lernen, das an Tun geknüpft ist, bleibt hängen. Abstrakte Grundlagen ohne Anwendung schweben davon.
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Ja, ich fühle, dass es entscheidende Lücken gibt. Um es wirklich vollständig und tief zu verstehen.
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Wo fühlst du die Lücken?\par
    Versuch mir zu erklären – der Akt des Artikulierens, wo dein Verständnis zusammenbricht, ist selbst diagnostisch. Kannst du zum Beispiel nachvollziehen, was passiert, wenn du einen Prompt in ComfyUI eingibst und ein Bild zurückbekommst? An welchem Punkt wird es zur Black Box?
\end{claudemsg}
```

```{=latex}
\begin{usermsg}
Nur ganz leicht. Ich verstehe Bild- oder Video-Generierungsmodelle nicht ganz, und ich bin mir nicht sicher, wie interessiert ich daran bin.
\end{usermsg}
```

```{=latex}
\begin{usermsg}
Ich will eigentlich einen Lo-Fi Soviet Dystopian Winter Soundtrack mit KI erstellen. Vielleicht kann ich da anfangen.
\end{usermsg}
```

```{=latex}
\begin{claudemsg}
Das ist ein großartiger Einstiegspunkt – spezifisch, kreativ, und persönlich interessant. Du wirst mehr lernen, wenn du dem nachjagst, als wenn du dich zwingst, Image Diffusion zu verstehen, das dich nicht interessiert.\par
    Was weißt du bisher über KI-Musikgenerierung? Hast du dir schon Tools angeschaut, oder ist das noch im Ideenstadium?
\end{claudemsg}
```

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du verwendet?
\end{claudemsg}
\begin{usermsg}
Claude über die mobile App mit Spracheingabe, während einer 30-minütigen Autofahrt. Freisprechanlage an, quasi wie ein Telefonat.
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Ich wollte KI wirklich verstehen, nicht nur die Tools benutzen, sondern die Grundlagen. Was passiert mathematisch? Wie hängen Transformer, Diffusion Models und die Tools, die ich benutze, zusammen? Und wie lerne ich das am besten?
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? \emph{(Workflow)}
\end{claudemsg}
\begin{usermsg}
Ein komplett neuer Lernmodus. Statt Podcast oder YouTube-Video hatte ich ein personalisiertes Gespräch. Die KI hat sich an mein Level angepasst, ich konnte jederzeit nachfragen. Dialog statt passives Konsumieren.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle erlebt? \emph{(Autorschaft)}
\end{claudemsg}
\begin{usermsg}
Die Fragende. Ich habe die Richtung bestimmt, welche Themen, wie tief, welche Beispiele. Claude hat erklärt, aber ohne meine Fragen wäre das Gespräch nirgendwohin gegangen.
\end{usermsg}
\begin{claudemsg}
Wie hat die KI-Ausgabe deinen Prozess beeinflusst? \emph{(Iteration)}
\end{claudemsg}
\begin{usermsg}
Zum einen denke ich, dass ich mein bereits bekanntes Wissen noch einmal gestärkt habe und mich an ein paar Konzepte und historische Ereignisse erinnert habe, die ich schon vergessen hatte.\par
  Ich glaube aber, es könnte die Gefahr geben, dass ich der Illusion unterliege, dass ich es verstehe, obwohl ich es vielleicht nicht tue. Aber das Gleiche gilt ja auch, wenn ich mir ein Video anschaue.
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen? \emph{(Scheitern)}
\end{claudemsg}
\begin{usermsg}
Es lief noch nicht ganz so flüssig. Ich glaube, die Implementation von ChatGPT ist ein bisschen besser.\par
  Da wurde ich ein bisschen von der Autofahrt abgelenkt, ich würde das vielleicht nicht unbedingt noch einmal wiederholen. Wäre es jetzt wie ein Telefonat, dann ja, aber durch die Bugs lieber nicht. Aber sicherlich wird sich noch einiges tun in den nächsten Monaten.
\end{usermsg}
\begin{claudemsg}
Hat sich eine eigene 'Lern-Sprache' oder Methode entwickelt? \emph{(Ästhetik)}
\end{claudemsg}
\begin{usermsg}
Ich denke ich würde jetzt öfters im Dialog mit der KI versuchen Dinge zu lernen.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Am Ende des Eintrags hast du geschrieben: 'Die Theorie wird erst real, wenn ich sie anwende. Nächster Schritt: Der Lo-Fi Soundtrack.' – Hast du das inzwischen gemacht?
\end{claudemsg}
\begin{usermsg}
Ja, das Suno-Experiment war der direkte nächste Schritt. Claude hatte vorgeschlagen, KI-Verständnis an konkreten Projekten zu verankern statt abstrakt zu lernen, und genau das habe ich dann gemacht.
\end{usermsg}
```

\newpage

### Journal-Eintrag 6: Suno-Experiment (12. Januar 2026)

Heute ein kleines Experiment außerhalb des Kepler-Kontexts. Ich arbeite an einem anderen Projekt – einer prozedural generierten Spielwelt mit sowjetischer Ästhetik – und wollte testen, wie gut KI-Musikgenerierung für spezifische Game-Audio-Anforderungen funktioniert.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/933d23ad747ee06683a4acb4eb305f8d.png}
\caption{Houdini-Workflow mit den drei prozeduralen Generatoren: terrain\_generator, building\_generator und street\_generator für die sowjetisch inspirierte Spielwelt.}
\end{figure}
```
#### Prozeduralismus und Pattern-basiertes Design

Das Projekt "ПАНЕЛЬКИ" (Panelki) entsteht im Rahmen des Masterkurses über prozedurale Weltgenerierung – Virtuelle Welten. Der Kern: Anstatt jedes Gebäude und jeden Straßenzug manuell zu platzieren, definiere ich *Regeln* und *Muster*, aus denen die Welt emergent entsteht. In Houdini habe ich drei HDAs (Houdini Digital Assets) entwickelt:

- **terrain_generator**: Echte Höhendaten aus Neryungi, Russland (ALOS PALSAR DEM)
- **building_generator**: Modulare Soviet-Architektur aus OpenStreetMap-Footprints
- **street_generator**: Straßennetzwerke, die die Gebäude verbinden

Das Ergebnis ist eine dystopische Winterstadt, bevölkert von anthropomorphen Tierfiguren im Stil sowjetischer Zeichentrickfilme.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/60c01a9ba2c3a21ee20c7cd7e1eed892.png}
\caption{Konzeptkunst: Wolf unter einer Straßenlaterne im ewigen Winter. Konzeptbild erstellt mit Google Gemini (Nano Banana Pro).}
\end{figure}
```
#### KI-Musik

Was mich zu diesem Experiment gebracht hat: Ich sah ein Video eines Musikproduzenten in meinem YouTube Feed: viele kommerzielle Musik wird inzwischen KI-generiert – ohne dass es auffällt. Bei Bildern und Videos sehe ich noch deutliche Artefakte und Inkonsistenzen, KI-Werbespots werden regelmäßig "außeinandergenommen" und kritisiert. Bei Audio liegt es anscheinend für einen Laien nah am professionellen Standard. Suno ist wohl aktuell der Marktführer.


#### Das Experiment: Zwei Prompts

Das Video hatte mich inspiriert: ich brauchte Hintergrundmusik für meine Welt, die einen Pixel-look haben sollte: loopbar, atmosphärisch, sowjetisch-dystopisch. Keine Vocals. Also habe ich mich angemeldet und zwei Ansätze auf Suno getestet.

##### Versuch 1: "Lo-fi"

**Ergebnis:** Klingt zu kommerziell. "Lo-fi" war der falsche Begriff – ich wollte etwas Minimalistischeres, Roheres. Die generierte Beschreibung klingt besser als der tatsächliche Output.

##### Versuch 2: "Minimal"

**Ergebnis:** Nicht wirklich minimal. Der Fokus auf "pixel game soundtrack" hat dazu geführt, dass es teilweise wie ein Hero-Soundtrack aus einem Indie-Game klingt – nicht wie atmosphärische Hintergrundmusik für ein Spiel, in dem man explorativ durch eine verlassene Stadt läuft. Die Beschreibungen sind interessant ("aging arcade cabinet in an abandoned bunker"), ich finde der tatsächliche Sound geht in eine andere Richtung.

#### Die Beschreibungs-Output-Diskrepanz

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/66958d2a115e32fe38d093d0dd9da472.png}
\caption{Pixel-Art-Ästhetik: Das Schaf auf sowjetischer Straße – genau dieser Stil bräuchte passende Musik. Konzeptbild erstellt mit Midjourney v7.}
\end{figure}
```
Was auffällt: Suno generiert wunderbar detaillierte Beschreibungen. "Bitcrushed chiptune pads over a sparse, detuned synth choir" klingt exakt nach dem, was ich will. Der tatsächliche Output? Klingt halt etwas zu "polished", wie Stock-Music.

Das ist dasselbe Muster wie bei meinen ComfyUI-Experimenten mit Kepler: Ich wollte einen voxelisierten, pixeligen Charakter ohne erkennbares Gesicht. Die KI lieferte typisch KI-generiert aussehende Ergebnisse mit Uncanny-Valley-Gesichtern, obwohl der Prompt explizit "voxel", "blocky", "no face" enthielt.

**Die Erkenntnis:** Generalistische KI-Tools sind wahrscheinlich einfach auf kommerzielle Mehrheitsästhetik trainiert. Spezialisierte Nischen – Pixel-Game-Soundtracks, voxelisierte Charaktere – erfordern entweder sehr viele Iterationen, spezialisierte Fine-Tunes oder manuelle Nachbearbeitung

In diesem Fall hat das Tool die Anforderungen nicht erfüllt – der Output schien für mich "handwerklich solide", aber stilistisch daneben, "too much". Für kommerzielle Mainstream-Musik mag das funktionieren, für spezialisierte Nischen braucht es nach wie vor menschliche Expertise oder zumindest erhebliche Nachbearbeitung.

#### Technische Notizen

Suno bietet zwei Modelle:
- **v4.5**: Kostenlos, 50 Credits/Tag, "previously unimaginable production capabilities"
- **v5**: $30/Monat (Pro), noch höhere Qualität, v5-Samples werden parallel angeboten

Ich habe beide Modelle getestet: Die "Frozen"-Tracks (Frozen Terminals, Frozen Relay) sind mit v5 (beta) generiert – im Free Trial allerdings auf eine Minute begrenzt. Red Snow Protocol und Iron Frost Protocol stammen aus v4.5 und sind länger, gefallen mir aber weniger – **warum genau, kann ich nicht sagen**.

#### Reflexionsinterview

```{=latex}
\chatsection{Kontext}
\begin{claudemsg}
Welche Tools hast du verwendet?
\end{claudemsg}
\begin{usermsg}
Suno AI, im Browser. Ich habe sowohl v4.5 (kostenlos) als auch v5 (beta, im Free Trial) getestet.
\end{usermsg}
\begin{claudemsg}
Was wolltest du erreichen?
\end{claudemsg}
\begin{usermsg}
Einen loopbaren, atmosphärischen Soundtrack für meine sowjetisch-dystopische Spielwelt Panelki. Minimal, kalt, pixelig. Keine Vocals, eher Hintergrundmusik zum Explorieren.
\end{usermsg}
\chatsection{Die 5 Forschungsfragen}
\begin{claudemsg}
Hat sich dein Arbeitsablauf verändert? \emph{(Workflow)}
\end{claudemsg}
\begin{usermsg}
Es war ein schneller Prototyp. Ich konnte Ideen testen, ohne selbst Musik produzieren zu können. Prompt rein, Ergebnis in Sekunden. Aber genau diese Geschwindigkeit hat auch gezeigt, wie wenig Kontrolle ich über das Ergebnis hatte.
\end{usermsg}
\begin{claudemsg}
Wie hast du deine Rolle erlebt? \emph{(Autorschaft)}
\end{claudemsg}
\begin{usermsg}
Enttäuschend. Ich war nur Kurator:in, ich konnte auswählen, aber nicht gestalten. Kein Regler, kein Parameter, den ich hätte drehen können. Nur der Prompt, und der wurde offensichtlich anders interpretiert als gemeint.
\end{usermsg}
\begin{claudemsg}
Wie hat die schnelle KI-Ausgabe deinen Prozess beeinflusst? \emph{(Iteration)}
\end{claudemsg}
\begin{usermsg}
In dem Fall habe ich das Ganze ja verworfen, weil ich vermute, dass ich mit dem Modell nicht sonderlich weit komme.
\end{usermsg}
\begin{claudemsg}
Was ist schiefgelaufen? \emph{(Scheitern)}
\end{claudemsg}
\begin{usermsg}
Der Output klang zu kommerziell, zu poliert, wie Stock-Music. Das Frustrierende: Die generierten \emph{Beschreibungen} trafen genau meinen Geschmack ('Bitcrushed chiptune pads over a sparse, detuned synth choir'), aber die Musik selbst klang völlig anders.
\end{usermsg}
\begin{claudemsg}
Hat sich eine eigene musikalische/ästhetische Sprache entwickelt? \emph{(Ästhetik)}
\end{claudemsg}
\begin{usermsg}
Nein, komplett verfehlt. Ich wollte minimal-dystopisch, bekommen habe ich generischen Indie-Game-Sound. Das zeigt mir, dass Suno auf kommerzielle Mehrheitsästhetik trainiert ist. Meine Nische liegt einfach außerhalb dessen, was das Modell liefern kann.
\end{usermsg}
\chatsection{Synthese}
\begin{claudemsg}
Im Eintrag schreibst du, dass Sunos generierte Beschreibungen ('Bitcrushed chiptune pads over a sparse, detuned synth choir') besser klingen als die Musik selbst. Ist das frustrierend – oder zeigt es, dass du die KI vielleicht als Text-Generator für Briefings an echte Musiker nutzen könntest?
\end{claudemsg}
\begin{usermsg}
Also zeigt mir, dass für die Vision, die ich hatte, gibt es Begriffe, und es gibt mir Hoffnung, dass ich entweder bereits existierende Musik mit diesen Begriffen finden kann, ein anderes Tool verwenden könnte, um so etwas zu generieren, oder es einem echten Musiker geben könnte. Es ist sehr frustrierend.
\end{usermsg}
```

\newpage

### Journal-Eintrag 7: Kepler-Recherche, zweiter Versuch (25. Januar 2026)

Im November hatte ich mit ComfyUI versucht, konsistente Bilder von Kepler zu generieren. Es war frustrierend. Die KI interpretierte hartnäckig Gesichter in seine Voxel-Maske hinein — egal, was ich promptete. Face Detailer, PuLID, InstantID: alle auf menschliche Gesichter trainiert, alle unfähig, einen Charakter ohne Gesicht zu respektieren.

```{=latex}
\begin{figure}[H]
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/c14f431db7abee98fdf3de87d5d9412c.png}\\[2pt]{\small\color{darktext!70} Kepler in seiner Voxel-Form — kein Gesicht, das ist Absicht.}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/cf06af54acd2340a567091d4c11d91f3.png}\\[2pt]{\small\color{darktext!70} Generiertes Bild aus dem ComfyUI-Experiment: Die KI hat ein Gesicht in die Maske interpretiert.}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{_img_cache/db4e1c875de041157bca5d4dbbc99f58.png}\\[2pt]{\small\color{darktext!70} Noch ein Versuch — wieder ein Gesicht, wo keines sein sollte.}
\end{minipage}
\end{figure}
```
Seitdem habe ich das Thema nicht losgelassen. In den Wochen dazwischen habe ich andere Dinge gemacht — Suno ausprobiert, mit Claude über KI-Grundlagen geredet, die Thesis-Struktur weitergedacht. Aber im Hinterkopf blieb dieses Bild: Kepler mit einem fremden Gesicht. Es fühlte sich an, als hätte das System meinen Charakter nicht nur falsch dargestellt, sondern ihn *korrigiert*. Als wäre ein gesichtsloses Wesen ein Fehler, den die KI beheben muss.

Ich wollte wissen: Liegt es an meinem Workflow? An den Tools? Oder ist das ein grundsätzliches Problem, das alle generativen KI-Modelle betrifft, wenn man nicht-normative Charaktere abbilden will?

Also habe ich mich einen Nachmittag hingesetzt und systematisch recherchiert, mit Claude als Sparringspartner.

#### Face-Tools sind das Kernproblem

Mein erster Fehler im November war nicht die Wahl von ComfyUI — es war die Wahl der Face-Tools. [PuLID](https://github.com/ToTheBeginning/PuLID), [InstantID](https://github.com/InstantID/InstantID) und Face Detailer sind alle auf menschliche Gesichter trainiert. Sie erwarten Augen, Nase, Mund. Kepler hat eine strukturierte Maske ohne erkennbare Gesichtszüge. Das System *will* ein Gesicht sehen, weil die Trainingsdaten sagen: Da, wo ein Kopf ist, muss ein Gesicht sein.

Das ist keine Limitierung meines Prompts. Das ist eine Limitierung der Repräsentation in den Trainingsdaten.

#### Nano Banana — nicht was ich dachte

Bei der Recherche bin ich auf [ComfyUI-NanoBanano](https://github.com/ShmuelRonen/ComfyUI-NanoBanano) gestoßen, das Google Gemini 2.5 Flash Image in ComfyUI integriert. Klingt vielversprechend, ist aber ein 2D-Bildgenerator — kein 3D-Tool. Für Kepler-Konzeptbilder potenziell nützlich, aber nicht für das eigentliche Problem: einen voxelisierten 3D-Charakter konsistent zu generieren.

#### 2D-Workaround: ControlNet Depth + Inpainting

Ein vielversprechender Ansatz: Statt Face-Tools zu nutzen, kann ich über Depth Maps arbeiten. [Flux.1-dev-Controlnet-Depth](https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Depth) extrahiert die räumliche Struktur eines Bildes und nutzt sie als Kontrollsignal — ohne das Gesicht interpretieren zu müssen. Kombiniert mit Inpainting könnte ich Kepler-Referenzen mit Tiefenkarte einspeisen und die KI nur den Stil und die Umgebung generieren lassen, während die Silhouette erhalten bleibt.

Das umgeht das Face-Problem elegant: Die Depth Map sieht nur Geometrie, keine Gesichtszüge.

#### 3D-native Modelle — der eigentliche Hoffnungsschimmer

Was mich wirklich überrascht hat: Es gibt inzwischen Modelle, die nativ in 3D arbeiten und teilweise explizit mit Voxel-Repräsentationen.

**[TRELLIS 2](https://github.com/microsoft/TRELLIS.2)** (Microsoft, MIT-Lizenz) arbeitet mit einer voxel-nativen Repräsentation — sogenannten *O-Voxels* (field-free sparse voxels). Das Modell generiert 3D-Assets aus Text oder Bildern und denkt dabei nativ in Voxeln. Keine 2D-Projektion, keine Face Detection, keine implizite Annahme über Gesichter.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/24c838c2058dd1fe25e9b14df82018db.png}
\caption{TRELLIS 2 generiert 3D-Modelle nativ aus Voxel-Repräsentationen — genau die Datenstruktur, in der Kepler existiert.}
\end{figure}
```
**[XCube](https://github.com/nv-tlabs/XCube)** (NVIDIA) nutzt hierarchische Voxel-Strukturen für hochauflösende 3D-Generierung. Ein Forschungsprojekt, kein Produkt — aber der Ansatz, über Voxel-Hierarchien zu arbeiten, ist für Kepler natürlicher als jede 2D-basierte Methode.

**[Hunyuan3D-2](https://github.com/Tencent-Hunyuan/Hunyuan3D-2)** (Tencent, Open Source) ist ein vollständiges 3D-Generierungs-Ökosystem mit Text-to-3D und Image-to-3D. Weniger voxel-spezifisch, aber als offenes Modell experimentierfreundlich.

```{=latex}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_img_cache/664b4f3daeeff10d13a29bc889ea4850.png}
\caption{Hunyuan3D-2 (HY3D) — Tencents Open-Source 3D-Generierungsmodell.}
\end{figure}
```
**[VoxAI](https://www.voxelai.ai/)** positioniert sich explizit als voxel-spezifisches KI-Tool. Noch sehr früh und wenig dokumentiert, aber allein die Existenz eines Tools, das sich auf Voxel-Generierung spezialisiert, zeigt: Es gibt Nachfrage für genau diesen Anwendungsfall.

#### TroublingGAN — ein verwandtes Artistic-Research-Projekt

Bei der Recherche bin ich auf ein Projekt gestoßen, das mein eigenes Thema spiegelt: **[TroublingGAN](https://www.researchcatalogue.net/view/1486468/1586300)** von Lenka Hámošová und Pavol Rusnák (JAR 31). Sie haben GANs bewusst an nicht-standardisierten Körpern scheitern lassen und das Scheitern selbst als ästhetische Strategie dokumentiert.

Ich habe fast aufgehört zu scrollen, als ich das gefunden habe. Jemand anderes hatte dasselbe Problem — und hat daraus Kunst gemacht, statt es als Bug abzutun. Das ist exakt mein Thema: Was passiert, wenn generative KI auf einen Charakter trifft, der die impliziten Normen der Trainingsdaten bricht? Bei mir ist es das fehlende Gesicht. Bei Hámošová & Rusnák waren es nicht-normative Körper. Das Ergebnis ist dasselbe: Das Modell *normalisiert*, weil es auf Normalität trainiert wurde.

#### Die Kern-Erkenntnis

Die Repräsentationsform bestimmt, ob nicht-normative Ästhetik überlebt.

2D-Bildgeneratoren arbeiten mit Pixeln und impliziten Annahmen über Gesichter, Körper, Proportionen. Sie *wollen* normalisieren, weil ihre Trainingsdaten normativ sind. 3D-native Modelle arbeiten mit Geometrie — Voxel, Meshes, Point Clouds. Sie haben keine implizite Erwartung, dass an einer bestimmten Stelle ein Gesicht sein muss.

Das bedeutet: Nicht die KI als solche ist das Problem. Die Repräsentationsebene ist es. Und die kann ich wählen.

#### Tool-Vergleich

| Tool | Typ | Voxel-nativ | Face-Bias | Lizenz | Status |
|------|-----|:-----------:|:---------:|--------|--------|
| ComfyUI + Face Tools | 2D | Nein | Hoch | Open Source | Gescheitert |
| ControlNet Depth | 2D | Nein | Niedrig | Open Source | Vielversprechend |
| TRELLIS 2 | 3D | **Ja** | Keiner | MIT | Nächster Test |
| XCube | 3D | **Ja** | Keiner | Research | Beobachten |
| Hunyuan3D-2 | 3D | Nein | Niedrig | Open Source | Ausprobieren |
| VoxAI | 3D | **Ja** | Keiner | Kommerziell | Beobachten |

#### Ausblick

Mein nächstes Experiment wird ein Hybrid-Ansatz: TRELLIS 2 für 3D-Generierung von Kepler-ähnlichen Voxel-Formen, kombiniert mit dem ControlNet-Depth-Inpainting-Workflow für 2D-Konzeptbilder. Statt das Face-Problem zu lösen, umgehe ich es — indem ich Tools wähle, die gar nicht erst nach Gesichtern suchen.

Ob das funktioniert, weiß ich nicht. Vielleicht scheitert TRELLIS 2 auf eine völlig andere Weise. Aber zum ersten Mal habe ich eine Hypothese, die ich testen kann — statt blind in Workflows zu stolpern, die nicht für Kepler gemacht sind.

\newpage

# Eidesstattliche Erklärung

Ich versichere hiermit, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Stellen der Arbeit, die wörtlich oder sinngemäß aus Veröffentlichungen oder aus anderweitigen fremden Äußerungen entnommen wurden, sind als solche einzeln kenntlich gemacht.

Der Einsatz generativer KI-Systeme (Claude, Cursor) bei der Erstellung dieser Arbeit ist im Abschnitt "Einsatz von KI in dieser Arbeit" vollständig offengelegt.

\vspace{2cm}

Detmold, Februar 2026

\vspace{2cm}

\noindent\rule{6cm}{0.4pt}

Jennifer Meier
